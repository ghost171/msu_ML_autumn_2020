{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритмы интеллектуальной обработки больших объемов данных\n",
    "## Домашнее задание №2: Линейные модели\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <hr\\>\n",
    "**Общая информация**\n",
    "\n",
    "**Срок сдачи:** 09 ноября 18:00 Сдача **очная** на онлайн занятии. <br\\>\n",
    "\n",
    "\n",
    "Используйте данный Ipython Notebook при оформлении домашнего задания.\n",
    "\n",
    "Присылать ДЗ необходимо в виде ссылки на свой github репозиторий на почту ml1.sphere@mail.ru с указанием темы в следующем формате:\n",
    "\n",
    "[ML0920, Задание 2] Фамилия Имя.\n",
    "\n",
    "\n",
    "\n",
    "**Штрафные баллы:**\n",
    "\n",
    "1. Невыполнение PEP8 -1 балл\n",
    "2. Отсутствие фамилии в имени скрипта (скрипт должен называться по аналогии со stroykova_hw2.ipynb) -1 балл\n",
    "3. Все строчки должны быть выполнены. Нужно, чтобы output команды можно было увидеть уже в git'е. В противном случае -1 балл\n",
    "4. При оформлении ДЗ нужно пользоваться данным файлом в качестве шаблона. Не нужно удалять и видоизменять написанный код и текст, если явно не указана такая возможность. В противном случае -1 балл\n",
    "<hr\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%pycodestyle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здравствуйте, уважаемые студенты! \n",
    "\n",
    "В этом задании мы будем реализовать линейные модели. Необходимо реализовать линейную и логистическую регрессии с L2 регуляризацией"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теоретическое введение\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Линейная регрессия решает задачу регрессии и оптимизирует функцию потерь MSE \n",
    "\n",
    "$$L(w) =  \\frac{1}{N}\\left[\\sum_i (y_i - a_i) ^ 2 \\right], $$ где $y_i$ $-$ целевая функция,  $a_i = a(x_i) =  \\langle\\,x_i,w\\rangle ,$ $-$ предсказание алгоритма на объекте $x_i$, $w$ $-$ вектор весов (размерности $D$), $x_i$ $-$ вектор признаков (такой же размерности $D$).\n",
    "\n",
    "Не забываем, что здесь и далее  мы считаем, что в $x_i$ есть тождественный вектор единиц, ему соответствует вес $w_0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия является линейным классификатором, который оптимизирует так называемый функционал log loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L(w) = - \\frac{1}{N}\\left[\\sum_i y_i \\log a_i + ( 1 - y_i) \\log (1 - a_i) \\right],$$\n",
    "где  $y_i  \\in \\{0,1\\}$ $-$ метка класса, $a_i$ $-$ предсказание алгоритма на объекте $x_i$. Модель пытается предсказать апостериорую вероятность объекта принадлежать к классу \"1\":\n",
    "$$ p(y_i = 1 | x_i) = a(x_i) =  \\sigma( \\langle\\,x_i,w\\rangle ),$$\n",
    "$w$ $-$ вектор весов (размерности $D$), $x_i$ $-$ вектор признаков (такой же размерности $D$).\n",
    "\n",
    "Функция $\\sigma(x)$ $-$ нелинейная функция, пероводящее скалярное произведение объекта на веса в число $\\in (0,1)$ (мы же моделируем вероятность все-таки!)\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$$\n",
    "\n",
    "Если внимательно посмотреть на функцию потерь, то можно заметить, что в зависимости от правильного ответа алгоритм штрафуется или функцией $-\\log a_i$, или функцией $-\\log (1 - a_i)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто для решения проблем, которые так или иначе связаны с проблемой переобучения, в функционал качества добавляют слагаемое, которое называют ***регуляризацией***. Итоговый функционал для линейной регрессии тогда принимает вид:\n",
    "\n",
    "$$L(w) =  \\frac{1}{N}\\left[\\sum_i (y_i - a_i) ^ 2 \\right] + \\frac{1}{C}R(w) $$\n",
    "\n",
    "Для логистической: \n",
    "$$L(w) = - \\frac{1}{N}\\left[\\sum_i y_i \\log a_i + ( 1 - y_i) \\log (1 - a_i) \\right] +  \\frac{1}{C}R(w)$$\n",
    "\n",
    "Самое понятие регуляризации введено основателем ВМК академиком Тихоновым https://ru.wikipedia.org/wiki/Метод_регуляризации_Тихонова\n",
    "\n",
    "Идейно методика регуляризации заключается в следующем $-$ мы рассматриваем некорректно поставленную задачу (что это такое можно найти в интернете), для того чтобы сузить набор различных вариантов (лучшие из которых будут являться переобучением ) мы вводим дополнительные ограничения на множество искомых решений. На лекции Вы уже рассмотрели два варианта регуляризации.\n",
    "\n",
    "$L1$ регуляризация:\n",
    "$$R(w) = \\sum_{j=1}^{D}|w_j|$$\n",
    "$L2$ регуляризация:\n",
    "$$R(w) =  \\sum_{j=1}^{D}w_j^2$$\n",
    "\n",
    "С их помощью мы ограничиваем модель в  возможности выбора каких угодно весов минимизирующих наш лосс, модель уже не сможет подстроиться под данные как ей угодно. \n",
    "\n",
    "Вам нужно добавить соотвествущую Вашему варианту $L2$ регуляризацию.\n",
    "\n",
    "И так, мы поняли, какую функцию ошибки будем минимизировать, разобрались, как получить предсказания по объекту и обученным весам. Осталось разобраться, как получить оптимальные веса. Для этого нужно выбрать какой-то метод оптимизации.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный спуск является самым популярным алгоритмом обучения линейных моделей. В этом задании Вам предложат реализовать стохастический градиентный спуск или  мини-батч градиентный спуск (мини-батч на русский язык довольно сложно перевести, многие переводят это как \"пакетный\", но мне не кажется этот перевод удачным). Далее нам потребуется определение **эпохи**.\n",
    "Эпохой в SGD и MB-GD называется один проход по **всем** объектам в обучающей выборки.\n",
    "* В SGD градиент расчитывается по одному случайному объекту. Сам алгоритм выглядит примерно так:\n",
    "        1) Перемешать выборку\n",
    "        2) Посчитать градиент функции потерь на одном объекте (далее один объект тоже будем называть батчем)\n",
    "        3) Сделать шаг спуска\n",
    "        4) Повторять 2) и 3) пока не пройдет максимальное число эпох.\n",
    "* В Mini Batch SGD - по подвыборке объектов. Сам алгоритм выглядит примерно так::\n",
    "        1) Перемешать выборку, выбрать размер мини-батча (от 1 до размера выборки)\n",
    "        2) Почитать градиент функции потерь по мини-батчу (не забыть поделить на  число объектов в мини-батче)\n",
    "        3) Сделать шаг спуска\n",
    "        4) Повторять 2) и 3) пока не пройдет максимальное число эпох.\n",
    "* Для отладки алгоритма реализуйте возможность  вывода средней ошибки на обучении модели по объектам (мини-батчам). После шага градиентного спуска посчитайте значение ошибки на объекте (или мини-батче), а затем усредните, например, по ста шагам. Если обучение проходит корректно, то мы должны увидеть, что каждые 100 шагов функция потерь уменьшается. \n",
    "* Правило останова - максимальное количество эпох\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теоретические вопросы (2 балла)\n",
    "В этой части Вам будут предложены теоретичские вопросы и задачи по теме. Вы, конечно, можете списать их у своего товарища или найти решение в интернете, но учтите, что они обязательно войдут в теоретический коллоквиум. Лучше разобраться в теме сейчас и успешно ответить на коллоквиуме, чем списать, не разобравшись в материале, и быть терзаемым совестью. \n",
    "\n",
    "\n",
    "Формулы надо оформлять в формате **LaTeX**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 1. Градиент для линейной регрессии.\n",
    "* Выпишите формулу обновления весов для линейной регрессии с L2 регуляризацией для мини-батч градиентого спуска размера $n$:\n",
    "\n",
    "$$ w_{new} = w_{old} - ... $$\n",
    "\n",
    " Отнеситесь к этому пункту максимально серьезно, это Вам нужно будет реализовать в задании.\n",
    " \n",
    "Проанализруйте итоговую формулу градиента - как  интуитивно можно  описать, чему равен градиент?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше решение здесь***\n",
    "\n",
    "$$ \\Large L\\left(\\vec{w}\\right) =  \\frac{1}{2N}\\left[\\sum_i (y_i - a_i) ^ 2 \\right] + \\frac{1}{2C}R(w) $$\n",
    "\n",
    "$$ \\Large L\\left(\\vec{w}\\right) = \\frac{1}{2} \\left(\\vec{y} - X \\vec{w}\\right)^T \\left(\\vec{y} - X \\vec{w}\\right) + \\frac{1}{2C} \\vec{w}^T \\vec{w} $$\n",
    "\n",
    "$$ \\Large \\frac{\\partial L}{\\partial \\vec{w}} = \\Large \\frac{\\partial}{\\partial \\vec{w}} \\left(\\frac{1}{2} \\left(\\vec{y} - X \\vec{w}\\right)^T \\left(\\vec{y} - X \\vec{w}\\right) + \\frac{1}{2C} \\vec{w}^T \\vec{w}\\right) \\\\ = \\frac{\\partial}{\\partial \\vec{w}}\\left( \\frac{1}{2} \\left( \\vec{y}^T \\vec{y} -2\\vec{y}^T X \\vec{w} + \\vec{w}^T X^T X \\vec{w}\\right) + \\frac{1}{2C} \\vec{w}^T \\vec{w} \\right) \\\\ = -X^T \\vec{y} + X^T X \\vec{w} + \\frac{1}{C} \\vec{w}  $$\n",
    "\n",
    "$$ \\Large \\frac{\\partial L}{\\partial \\vec{w}} = 0 $$\n",
    "\n",
    "$$ \\Large -X^T \\vec{y} + X^T X \\vec{w} + \\frac{1}{C} \\vec{w} = 0 $$\n",
    "\n",
    "$$ \\Large  X^T X \\vec{w} + \\frac{1}{C} \\vec{w} = X^T \\vec{y} $$\n",
    "\n",
    "$$ \\Large (X^T X + \\frac{1}{C} ) \\vec{w} = X^T \\vec{y} $$\n",
    "\n",
    "$$ \\Large \\vec{w} = \\left(X^T X + \\frac{1}{C} E\\right)^{-1} X^T \\vec{y} $$\n",
    "\n",
    "$$ \\Large w_{new} = w_{old} -  \\sum_{i=1}^{n} \\frac{1}{k_{i}}(\\left(X_{i}^T X_{i} + \\frac{1}{C} E\\right)^{-1} X_{i}^T \\vec{y}) $$\n",
    "\n",
    "k_{i} - количество объектов в текущем мини бэтче \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 2. Градиент для логистической регрессии.\n",
    "* Выпишите формулу обновления весов для логистической регрессии с L2 регуляризацией  для мини-батч градиентого спуска размера $n$:\n",
    "\n",
    "$$ w_{new} = w_{old} - ... $$\n",
    "\n",
    " Отнеситесь к этому пункту максимально серьезно, это Вам нужно будет реализовать в задании.\n",
    " \n",
    "Проанализруйте итоговую формулу градиента - как  интуитивно можно  описать, чему равен градиент? Как соотносится этот градиент с градиентом, возникающий в задаче линейной регрессии?\n",
    "\n",
    "Подсказка: Вам градиент, которой получается если “в лоб” продифференцировать,  надо немного преобразовать.\n",
    "Надо подставить, что $1 - \\sigma(w,x) $ это  $1 - a(x_i)$, а  $-\\sigma(w,x)$ это $0 - a(x_i)$.  Тогда получится свести к одной красивой формуле с линейной регрессией, которую программировать будет намного проще."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше решение здесь***\n",
    "\n",
    "$$ \\Large L(w) = - \\frac{1}{N} \\sum_{i=1}^{N} [ y_i \\log \\sigma(w_i x_i) + ( 1 - y_i) \\log (1 -  \\sigma(w_i x_i))] +  \\frac{1}{2C}R(w) $$\n",
    "\n",
    "$$ \\Large L(w) = - \\frac{1}{N} \\sum_{i=1}^{N} [ y_i \\log \\sigma(w_i x_i) + ( 1 - y_i) \\log (1 - \\sigma(w_i x_i)] +  \\frac{1}{2C}\\sum_{i=1}^{N}w_i^2 $$\n",
    "\n",
    "$$ \\Large L(w) = - \\frac{1}{N} \\sum_{i=1}^{N} [ y_i \\log \\frac{1}{1 + \\exp(-w_i x_i)} + ( 1 - y_i) \\log (1 - \\frac{1}{1 + \\exp(-w_i x_i)}] +  \\frac{1}{2C}\\sum_{i=1}^{N}w_i^2 $$\n",
    "\n",
    "$$ \\Large \\frac{\\partial \\sigma(x)}{\\partial x} = \\sigma(x) ( 1 - \\sigma(x)) $$\n",
    "\n",
    "$$ \\Large \\frac{\\partial L(w)}{\\partial w_i} = \\frac{\\partial }{\\partial w_i} ( - \\frac{1}{N} \\sum_{i=1}^{N} [ y_i \\log \\sigma(w_i x_i) + ( 1 - y_i) \\log (1 - \\sigma(w_i x_i)] +  \\frac{1}{2C}\\sum_{i=1}^{N}w_i^2 ) $$\n",
    "\n",
    "$$ \\Large \\frac{\\partial L(w)}{\\partial w_i} = (  \\frac{1}{N}  [ -y \\frac{1}{\\sigma(\\vec{w}x)} \\frac{\\partial }{\\partial w_i} ( \\sigma(\\vec{w} x)) - \\frac{( 1 - y )}{1 - \\sigma({\\vec{w}x})} \\frac{\\partial}{\\partial w_i} (1 - \\sigma(\\vec{w} x ))]) +  \\frac{1}{C} \\vec{w} $$\n",
    "\n",
    "$$ \\Large \\frac{\\partial L(w)}{\\partial w_i} = (  \\frac{1}{N}  [ y \\frac{1}{\\sigma(\\vec{w}x)}  + \\frac{( 1 - y )}{1 - \\sigma({\\vec{w}x})}]  (\\frac{\\partial} {\\partial w_i} \\sigma{(\\vec{w} x)}))+  \\frac{1}{C} \\vec{w} $$\n",
    "\n",
    "$$ \\Large \\frac{\\partial L(w)}{\\partial w_i} = (- \\frac{1}{N} [\\frac{y - \\sigma{(\\vec{w}x})}{(1 - \\sigma{(\\vec{w} x}))\\sigma{(\\vec{w} x})}] ) \\sigma{(\\vec{w} x)} [1 - \\sigma{(\\vec{w} x)}]x_i + \\frac{1}{C} \\vec{w} $$\n",
    "\n",
    "$$ \\Large \\frac{\\partial L(w)}{\\partial w_i} = [\\sigma{(\\vec{w} x}) - y] x_i + \\frac{1}{C} \\vec{w} = 0 $$\n",
    "\n",
    "$$ \\Large w_{new} = w_{old} -  \\alpha ([\\sigma{(\\vec{w} x}) - y] x_i + \\frac{1}{C} \\vec{w}) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 3. Точное решение линейной регрессии\n",
    "\n",
    "На лекции было показано, что точное решение линейной регрессии имеет вид $w = (X^TX)^{-1}X^TY $. \n",
    "* Покажите, что это действительно является точкой минимума в случае, если матрица X имеет строк не меньше, чем столбцов и имеет полный ранг. Подсказка: посчитайте Гессиан и покажите, что в этом случае он положительно определен. \n",
    "* Выпишите точное решение для модели с $L2$ регуляризацией. Как L2 регуляризация помогает с точным решением где матрица X имеет линейно зависимые признаки?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше решение здесь***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 4.  Предсказываем вероятности.\n",
    "\n",
    "Когда говорят о логистической регрессии, произносят фразу, что она \"предсказывает вероятности положительного класса\". Давайте разберемся, что же за этим стоит. Посчитаем математическое ожидание функции потерь и проверим, что предсказание алгоритма, оптимизирующее это мат. ожидание, будет являться вероятностью положительного класса. \n",
    "\n",
    "И так, функция потерь на объекте $x_i$, который имеет метку $y_i \\in \\{0,1\\}$  для предсказания $a(x_i)$ равна:\n",
    "$$L(y_i, b) =-[y_i == 1] \\log a(x_i)  - [y_i == 0] \\log(1 - a(x_i)) $$\n",
    "\n",
    "Где $[]$ означает индикатор $-$ он равен единице, если значение внутри него истинно, иначе он равен нулю. Тогда мат. ожидание при условии конкретного $x_i$  по определение мат. ожидания дискретной случайной величины:\n",
    "$$E(L | x_i) = -p(y_i = 1 |x_i ) \\log a(x_i)  - p(y_i = 0 | x_i) \\log( 1 - a(x_i))$$\n",
    "* Докажите, что значение $a(x_i)$, минимизирующее данное мат. ожидание, в точности равно $p(y_i = 1 |x_i)$, то есть равно вероятности положительного класса.\n",
    "\n",
    "Подсказка: возможно, придется воспользоваться, что  $p(y_i = 1 | x_i) + p(y_i = 0 | x_i) = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше решение здесь***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 5.  Смысл регуляризации.\n",
    "\n",
    "Нужно ли в L1/L2 регуляризации использовать свободный член $w_0$ (который не умножается ни на какой признак)?\n",
    "\n",
    "Подсказка: подумайте, для чего мы вводим $w_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше решение здесь***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Реализация линейной модели (4 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зачем нужны батчи?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как Вы могли заметить из теоретического введения, что в случае SGD, что в случа mini-batch GD,  на каждой итерации обновление весов  происходит только по небольшой части данных (1 пример в случае SGD, batch примеров в случае mini-batch). То есть для каждой итерации нам *** не нужна вся выборка***. Мы можем просто итерироваться по выборке, беря батч нужного размера (далее 1 объект тоже будем называть батчом).\n",
    "\n",
    "Легко заметить, что в этом случае нам не нужно загружать все данные в оперативную память, достаточно просто считать батч с диска, обновить веса, считать диска другой батч и так далее. В целях упрощения домашней работы, прямо с диска  мы считывать не будем, будем работать с обычными numpy array. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Немножко про генераторы в Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея считывания данных кусками удачно ложится на так называемые ***генераторы*** из языка Python. В данной работе Вам предлагается не только разобраться с логистической регрессией, но  и познакомиться с таким важным элементом языка.  При желании Вы можете убрать весь код, связанный с генераторами, и реализовать логистическую регрессию и без них, ***штрафоваться это никак не будет***. Главное, чтобы сама модель была реализована правильно, и все пункты были выполнены. \n",
    "\n",
    "Подробнее можно почитать вот тут https://anandology.com/python-practice-book/iterators.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К генератору стоит относиться просто как к функции, которая порождает не один объект, а целую последовательность объектов. Новое значение из последовательности генерируется с помощью ключевого слова ***yield***. Ниже Вы можете насладиться  генератором чисел Фибоначчи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%pycodestyle\n",
    "def fib(max_iter=4):\n",
    "    a, b = 0, 1\n",
    "    iter_num = 0\n",
    "    while 1:\n",
    "        yield a\n",
    "        a, b = b, a + b\n",
    "        iter_num += 1\n",
    "        if iter_num == max_iter:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот так можно сгенерировать последовательность Фибоначчи. \n",
    "\n",
    "Заметьте, что к генераторам можно применять некоторые стандартные функции из Python, например enumerate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%pycodestyle\n",
    "new_generator = fib()\n",
    "for j, fib_val in enumerate(new_generator):\n",
    "    print(\"Fib num: \" + str(j) + \" fib values: \" + str(fib_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пересоздавая объект, можно сколько угодно раз генерировать заново последовательность. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%pycodestyle\n",
    "for i in range(0, 3):\n",
    "    new_generator = fib()\n",
    "    for j, fib_val in enumerate(new_generator):\n",
    "        print(\"Fib num: \" + str(j) + \" fib values: \" + str(fib_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А вот так уже нельзя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%pycodestyle\n",
    "new_generator = fib()\n",
    "for i in range(0, 3):\n",
    "    for j, fib_val in enumerate(new_generator):\n",
    "        print(\"Fib num: \" + str(j) + \" fib values: \" + str(fib_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Концепция крайне удобная для обучения  моделей $-$ у Вас есть некий источник данных, который Вам выдает их кусками, и Вам совершенно все равно откуда он их берет. Под ним может скрывать как массив в оперативной памяти, как файл на жестком диске, так и SQL база данных. Вы сами данные никуда не сохраняете, оперативную память экономите."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если Вам понравилась идея с генераторами, то Вы можете реализовать свой, используя прототип batch_generator. В нем Вам нужно выдавать батчи признаков и ответов для каждой новой итерации спуска. Если не понравилась идея, то можете реализовывать SGD или mini-batch GD без генераторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-dfb772f80841>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pycodestyle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'def batch_generator(X, y, shuffle=True, batch_size=1):\\n    \"\"\"\\n    Гератор новых батчей для обучения\\n    X          - матрица объекты-признаки\\n    y_batch    - вектор ответов\\n    shuffle    - нужно ли случайно перемешивать выборку\\n    batch_size - размер батча ( 1 это SGD, > 1 mini-batch GD)\\n    Генерирует подвыборку для итерации спуска (X_batch, y_batch)\\n    \"\"\"\\n    if (shuffle == True):\\n        np.random.shuffle(X)\\n    X_batch = \"\"\\n    y_batch = \"\"\\n    for i in range(batch_size, len(X[0]),batch_size):\\n        X_batch = X[i - batch_size:i]\\n        y_batch = y[i - batch_size:i]\\n        yield (X_batch, y_batch)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ghost/.local/lib/python3.6/site-packages/pycodestyle_magic.py\u001b[0m in \u001b[0;36mpycodestyle\u001b[0;34m(line, cell, auto)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m#logger.info(line)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# on windows drive path also contains :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;31m# do not subtract 1 for line for %%pycodestyle, inc pre py3.6 string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mauto\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "%%pycodestyle\n",
    "def batch_generator(X, y, shuffle=True, batch_size=1):\n",
    "    \"\"\"\n",
    "    Гератор новых батчей для обучения\n",
    "    X          - матрица объекты-признаки\n",
    "    y_batch    - вектор ответов\n",
    "    shuffle    - нужно ли случайно перемешивать выборку\n",
    "    batch_size - размер батча ( 1 это SGD, > 1 mini-batch GD)\n",
    "    Генерирует подвыборку для итерации спуска (X_batch, y_batch)\n",
    "    \"\"\"\n",
    "    if (shuffle == True):\n",
    "        np.random.shuffle(X)\n",
    "    X_batch = \"\"\n",
    "    y_batch = \"\"\n",
    "    for i in range(batch_size, len(X[0]),batch_size):\n",
    "        X_batch = X[i - batch_size:i]\n",
    "        y_batch = y[i - batch_size:i]\n",
    "        yield (X_batch, y_batch)\n"
   ]
  },
  {
   "source": [
    "%%pycodestyle\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Вычисляем значение сигмоида.\n",
    "    X - выход линейной модели\n",
    "    \"\"\"\n",
    "    sigm_value_x = 1 / (1 + np.exp(-x))\n",
    "    return sigm_value_x\n",
    "\n",
    "\n",
    "class MySGDClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, batch_generator, C=1,\n",
    "                 alpha=0.01, max_epoch=10, model_type='lin_reg'):\n",
    "        \"\"\"\n",
    "        batch_generator -- функция генератор, которой будем создавать батчи\n",
    "        C - коэф. регуляризации\n",
    "        alpha - скорость спуска\n",
    "        max_epoch - максимальное количество эпох\n",
    "        model_type - тим модели, lin_reg или log_reg\n",
    "        \"\"\"\n",
    "\n",
    "        self.C = C\n",
    "        self.alpha = alpha\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_generator = batch_generator\n",
    "        self.errors_log = {'iter' : [], 'loss' : []}\n",
    "        self.model_type = model_type\n",
    "\n",
    "\n",
    "    def calc_loss(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Считаем функцию потерь по батчу \n",
    "        X_batch - матрица объекты-признаки по батчу\n",
    "        y_batch - вектор ответов по батчу\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        \"\"\"\n",
    "        if (self.model_type == \"log_reg\"):\n",
    "            sum_error = 0\n",
    "            for i in range(len(y_batch)):\n",
    "                sum_error += y_batch[i] * np.log(sigmoid(np.dot(self.weights[i], X_batch[i]))) + (1 - y_batch[i]) * (np.log(1 - sigmoid(np.dot(self.weights[i], X_batch[i]))))\n",
    "            mean_error = (sum_error / float(len(X_batch))) + 1/float(self.C) * sum(self.weights ** 2)\n",
    "            return mean_error\n",
    "\n",
    "        if (self.model_type == \"lin_reg\"):\n",
    "            sum_error = 0\n",
    "            for i in range(len(y_batch)):\n",
    "                prediction_error = y_batch[i] - self.weights[i] * X_batch[i]\n",
    "                sum_error += (prediction_error ** 2)\n",
    "            \n",
    "            mean_error = (sum_error / float(len(X_batch))) + 1/float(self.C) * sum(self.weights ** 2)\n",
    "            return mean_error\n",
    "\n",
    "\n",
    "    def calc_loss_grad(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Считаем  градиент функции потерь по батчу (то что Вы вывели в задании 1)\n",
    "        X_batch - матрица объекты-признаки по батчу\n",
    "        y_batch - вектор ответов по батчу\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        \"\"\"\n",
    "        k = len(X_batch)\n",
    "        if (self.model_type == \"log_reg\"):\n",
    "            weights_for_grad = self.weights\n",
    "            ridge = (1 / self.C) * weights_for_grad\n",
    "            sigm = sigmoid(np.dot(weights_for_grad, X_batch.transpose()))\n",
    "            loss_grad = ((sigm) - y_batch) * X_batch.transpose()\n",
    "            loss_grad = loss_grad.sum(axis=1)\n",
    "            return loss_grad\n",
    "        if (self.model_type == \"lin_reg\"):\n",
    "            ridge = (1 / self.C) * np.eye(len(X_batch[0]))\n",
    "            traverse_matrix = np.linalg.inv(np.dot(X_batch.transpose(), X_batch) + ridge)\n",
    "            loss_grad = 1/float(k) * np.dot(np.dot(np.array(traverse_matrix), np.array(\n",
    "                X_batch.transpose())), np.array(y_batch))\n",
    "            loss_grad = loss_grad.sum(axis=0)\n",
    "            return loss_grad\n",
    "        return None\n",
    "\n",
    "\n",
    "    def initialize_weights(self, rows):\n",
    "        self.weights = np.random.random(rows+1)\n",
    "\n",
    "\n",
    "    def update_weights(self, new_grad):\n",
    "        \"\"\"\n",
    "        Обновляем вектор весов\n",
    "        new_grad - градиент по батчу\n",
    "        \"\"\"\n",
    "        self.weights -= new_grad\n",
    "        pass\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Обучение модели\n",
    "        X - матрица объекты-признаки\n",
    "        y - вектор ответов\n",
    "        '''\n",
    "\n",
    "\n",
    "        self.initialize_weights(len(X[0]))\n",
    "        if (self.model_type == \"log_reg\"):\n",
    "            self.weights = self.weights[1::]\n",
    "        for n in range(0, self.max_epoch):\n",
    "            if (self.model_type == \"lin_reg\"):\n",
    "                new_epoch_generator = self.batch_generator(X, y, shuffle = True, batch_size = 5)\n",
    "            else:\n",
    "                new_epoch_generator = self.batch_generator(X, y, shuffle=True, batch_size = 5)\n",
    "            for batch_num, new_batch in enumerate(new_epoch_generator):\n",
    "                X_batch = new_batch[0]\n",
    "                y_batch = new_batch[1]\n",
    "                batch_grad = self.alpha * self.calc_loss_grad(X_batch, y_batch)\n",
    "                self.update_weights(batch_grad)\n",
    "                batch_loss = self.calc_loss(X_batch, y_batch)\n",
    "                self.errors_log['iter'].append(batch_num)\n",
    "                self.errors_log['loss'].append(batch_loss)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Предсказание класса\n",
    "        X - матрица объекты-признаки\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        '''\n",
    "        weight_for_predict = self.weights[:len(X[0]):]\n",
    "        y_err = np.dot(X, weight_for_predict) \n",
    "        y_hat = np.array([0 if y_err[i] <= 0 else 1 for i in range(len(y_err))])\n",
    "        print(y_err)\n",
    "        return y_hat\n"
   ],
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b8e741882dcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pycodestyle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'from sklearn.base import BaseEstimator, ClassifierMixin\\n\\n\\ndef sigmoid(x):\\n    \"\"\"\\n    Вычисляем значение сигмоида.\\n    X - выход линейной модели\\n    \"\"\"\\n    sigm_value_x = 1 / (1 + np.exp(-x))\\n    return sigm_value_x\\n\\n\\nclass MySGDClassifier(BaseEstimator, ClassifierMixin):\\n\\n    def __init__(self, batch_generator, C=1,\\n                 alpha=0.01, max_epoch=10, model_type=\\'lin_reg\\'):\\n        \"\"\"\\n        batch_generator -- функция генератор, которой будем создавать батчи\\n        C - коэф. регуляризации\\n        alpha - скорость спуска\\n        max_epoch - максимальное количество эпох\\n        model_type - тим модели, lin_reg или log_reg\\n        \"\"\"\\n\\n        self.C = C\\n        self.alpha = alpha\\n        self.max_epoch = max_epoch\\n        self.batch_generator = batch_generator\\n        self.errors_log = {\\'iter\\' : [], \\'loss\\' : []}\\n        self.model_type = model_type\\n\\n\\n    def calc_loss(self, X_batch, y_batch):\\n        \"\"\"\\n        Считаем функцию потерь по батчу \\n        X_batch - матрица объекты-признаки по батчу\\n        y_batch - вектор ответов по батчу\\n        Не забудте тип модели (линейная или логистическая регрессия)!\\n        \"\"\"\\n        if (self.model_type == \"log_reg\"):\\n            sum_error = 0\\n            for i in range(len(y_batch)):\\n                sum_error += y_batch[i] * np.log(sigmoid(np.dot(self.weights[i], X_batch[i]))) + (1 - y_batch[i]) * (np.log(1 - sigmoid(np.dot(self.weights[i], X_batch[i]))))\\n            mean_error = (sum_error / float(len(X_batch))) + 1/float(self.C) * sum(self.weights ** 2)\\n            return mean_error\\n\\n        if (self.model_type == \"lin_reg\"):\\n            sum_error = 0\\n            for i in range(len(y_batch)):\\n                prediction_error = y_batch[i] - self.weights[i] * X_batch[i]\\n                sum_error += (prediction_error ** 2)\\n            \\n            mean_error = (sum_error / float(len(X_batch))) + 1/float(self.C) * sum(self.weights ** 2)\\n            return mean_error\\n\\n\\n    def calc_loss_grad(self, X_batch, y_batch):\\n        \"\"\"\\n        Считаем  градиент функции потерь по батчу (то что Вы вывели в задании 1)\\n        X_batch - матрица объекты-признаки по батчу\\n        y_batch - вектор ответов по батчу\\n        Не забудте тип модели (линейная или логистическая регрессия)!\\n        \"\"\"\\n        k = len(X_batch)\\n        if (self.model_type == \"log_reg\"):\\n            weights_for_grad = self.weights\\n            ridge = (1 / self.C) * weights_for_grad\\n            sigm = sigmoid(np.dot(weights_for_grad, X_batch.transpose()))\\n            loss_grad = ((sigm) - y_batch) * X_batch.transpose()\\n            loss_grad = loss_grad.sum(axis=1)\\n            return loss_grad\\n        if (self.model_type == \"lin_reg\"):\\n            ridge = (1 / self.C) * np.eye(len(X_batch[0]))\\n            traverse_matrix = np.linalg.inv(np.dot(X_batch.transpose(), X_batch) + ridge)\\n            loss_grad = 1/float(k) * np.dot(np.dot(np.array(traverse_matrix), np.array(\\n                X_batch.transpose())), np.array(y_batch))\\n            loss_grad = loss_grad.sum(axis=0)\\n            return loss_grad\\n        return None\\n\\n\\n    def initialize_weights(self, rows):\\n        self.weights = np.random.random(rows+1)\\n\\n\\n    def update_weights(self, new_grad):\\n        \"\"\"\\n        Обновляем вектор весов\\n        new_grad - градиент по батчу\\n        \"\"\"\\n        self.weights -= new_grad\\n        pass\\n\\n\\n    def fit(self, X, y):\\n        \\'\\'\\'\\n        Обучение модели\\n        X - матрица объекты-признаки\\n        y - вектор ответов\\n        \\'\\'\\'\\n\\n\\n        self.initialize_weights(len(X[0]))\\n        if (self.model_type == \"log_reg\"):\\n            self.weights = self.weights[1::]\\n        for n in range(0, self.max_epoch):\\n            if (self.model_type == \"lin_reg\"):\\n                new_epoch_generator = self.batch_generator(X, y, shuffle = True, batch_size = 5)\\n            else:\\n                new_epoch_generator = self.batch_generator(X, y, shuffle=True, batch_size = 5)\\n            for batch_num, new_batch in enumerate(new_epoch_generator):\\n                X_batch = new_batch[0]\\n                y_batch = new_batch[1]\\n                batch_grad = self.alpha * self.calc_loss_grad(X_batch, y_batch)\\n                self.update_weights(batch_grad)\\n                batch_loss = self.calc_loss(X_batch, y_batch)\\n                self.errors_log[\\'iter\\'].append(batch_num)\\n                self.errors_log[\\'loss\\'].append(batch_loss)\\n        return self\\n\\n\\n    def predict(self, X):\\n        \\'\\'\\'\\n        Предсказание класса\\n        X - матрица объекты-признаки\\n        Не забудте тип модели (линейная или логистическая регрессия)!\\n        \\'\\'\\'\\n        weight_for_predict = self.weights[:len(X[0]):]\\n        y_err = np.dot(X, weight_for_predict) \\n        y_hat = np.array([0 if y_err[i] <= 0 else 1 for i in range(len(y_err))])\\n        print(y_err)\\n        return y_hat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ghost/.local/lib/python3.6/site-packages/pycodestyle_magic.py\u001b[0m in \u001b[0;36mpycodestyle\u001b[0;34m(line, cell, auto)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m#logger.info(line)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# on windows drive path also contains :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;31m# do not subtract 1 for line for %%pycodestyle, inc pre py3.6 string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mauto\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустите обе регрессии на синтетических данных. \n",
    "\n",
    "\n",
    "Выведите полученные веса и нарисуйте разделяющую границу между классами (используйте только первых два веса для первых двух признаков X[:,0], X[:,1] для отображения в 2d пространство ).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%pycodestyle\n",
    "%matplotlib inline \n",
    "import seaborn as sns \n",
    "\n",
    "\n",
    "def plot_decision_boundary(clf, X, y):\n",
    "    min1, max1 = X[:, 0].min()-1, X[:, 0].max()+1 \n",
    "    min2, max2 = X[:, 1].min()-1, X[:, 1].max()+1\n",
    "    x1grid = np.arange(min1, max1, 0.1) \n",
    "    x2grid = np.arange(min2, max2, 0.1)\n",
    "    xx, yy = np.meshgrid(x1grid, x2grid)\n",
    "    r1, r2 = xx.flatten(), yy.flatten() \n",
    "    r1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))\n",
    "    grid = np.hstack((r1,r2))\n",
    "    clf.fit(X, y)\n",
    "    yhat = clf.predict(grid)\n",
    "    zz = yhat.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, zz, cmap='Paired')\n",
    "    for class_value in range(2):\n",
    "        row_ix = np.where(y == class_value)\n",
    "    plt.scatter(X[row_ix, 0], X[row_ix, 1], cmap='Paired')\n",
    "    pass"
   ]
  },
  {
   "source": [
    "%%pycodestyle\n",
    "np.random.seed(0)\n",
    "\n",
    "C1 = np.array([[0., -0.8], [1.5, 0.8]])\n",
    "C2 = np.array([[1., -0.7], [2., 0.7]])\n",
    "gauss1 = np.dot(np.random.randn(200, 2) + np.array([5, 3]), C1)\n",
    "gauss2 = np.dot(np.random.randn(200, 2) + np.array([1.5, 0]), C2)\n",
    "\n",
    "X = np.vstack([gauss1, gauss2])\n",
    "y = np.r_[np.ones(200), np.zeros(200)]\n",
    "\n",
    "clf = MySGDClassifier(batch_generator)\n",
    "\n",
    "plot_decision_boundary(clf, X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pycodestyle\n",
    "np.random.seed(0)\n",
    "\n",
    "C1 = np.array([[0., -0.8], [1.5, 0.8]])\n",
    "C2 = np.array([[1., -0.7], [2., 0.7]])\n",
    "gauss1 = np.dot(np.random.randn(200, 2) + np.array([5, 3]), C1)\n",
    "gauss2 = np.dot(np.random.randn(200, 2) + np.array([1.5, 0]), C2)\n",
    "\n",
    "X = np.vstack([gauss1, gauss2])\n",
    "y = np.r_[np.ones(200), np.zeros(200)]\n",
    "\n",
    "clf = MySGDClassifier(batch_generator, model_type=\"log_reg\")\n",
    "\n",
    "plot_decision_boundary(clf, X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее будем анализировать Ваш алгоритм. \n",
    "Для этих заданий используйте датасет ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%pycodestyle\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%pycodestyle\n",
    "\n",
    "X, y = make_classification(n_samples=100000, n_features=10,\n",
    "                           n_informative=4, n_redundant=0,\n",
    "                           random_state=123, class_sep=1.0,\n",
    "                           n_clusters_per_class=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покажите сходимости обеих регрессией на этом датасете: изобразите график  функции потерь, усредненной по $N$ шагам градиентого спуска, для разных `alpha` (размеров шага). Разные `alpha` расположите на одном графике. \n",
    "\n",
    "$N$ можно брать 10, 50, 100 и т.д. "
   ]
  },
  {
   "source": [
    "%%pycodestyle\n",
    "a = np.linspace(1, 10, num=30)\n",
    "x_row = np.array(a)\n",
    "y_row = []\n",
    "for i in a:\n",
    "    clf = MySGDClassifier(batch_generator, C=float(10), alpha=i)\n",
    "    clf.fit(X, y)\n",
    "    size = len(clf.errors_log['loss']) - 1\n",
    "    el_for_push = clf.errors_log['loss'][size]\n",
    "    y_row.append(el_for_push)\n",
    "fig = plt.subplots()\n",
    "plt.plot(x_row, y_row)"
   ],
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%pycodestyle\n",
    "a = np.linspace(1, 10, num=30)\n",
    "x_row = np.array(a)\n",
    "y_row = []\n",
    "for i in a:\n",
    "    clf = MySGDClassifier(batch_generator, C=float(\n",
    "        10), alpha=i, model_type=\"log_reg\")\n",
    "    clf.fit(X, y)\n",
    "    size = len(clf.errors_log['loss']) - 1\n",
    "    el_for_push = clf.errors_log['loss'][size]\n",
    "    y_row.append(el_for_push)\n",
    "fig = plt.subplots()\n",
    "plt.plot(x_row, y_row)"
   ]
  },
  {
   "source": [
    "Сходится к 0"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изобразите график среднего значения весов для обеих регрессий в зависимости от коеф. регуляризации С из `np.logspace(3, -3, 10)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%pycodestyle\n",
    "x_row = np.linspace(3, -3, 10)\n",
    "y_row = []\n",
    "for i in x_row:\n",
    "    clf = MySGDClassifier(batch_generator, C=i)\n",
    "    clf.fit(X, y)\n",
    "    y_row.append(clf.weights.mean())\n",
    "fig = plt.subplots()\n",
    "y_row = np.array(y_row)\n",
    "plt.plot(x_row, y_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pycodestyle\n",
    "x_row = np.linspace(3, -3, 10)\n",
    "y_row = []\n",
    "for i in x_row:\n",
    "    clf = MySGDClassifier(batch_generator, C=i, model_type=\"log_reg\")\n",
    "    clf.fit(X, y)\n",
    "    y_row.append(clf.weights.mean())\n",
    "fig = plt.subplots()\n",
    "y_row = np.array(y_row)\n",
    "plt.plot(x_row, y_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Довольны ли Вы, насколько сильно уменьшились Ваши веса? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Боевое применение (4  балла)\n",
    "\n",
    "**Защита данной части возможна только при преодолении в проекте бейзлайна Handmade baseline.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте применим модель на итоговом проекте! Датасет сделаем точно таким же образом, как было показано в project_overview.ipynb\n",
    "\n",
    "Применим обе регрессии, подберем для них параметры и сравним качество. Может быть Вы еще одновременно с решением домашней работы подрастете на лидерборде!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_to_title = {}\n",
    "with open('docs_titles.tsv') as f:\n",
    "    for num_line, line in enumerate(f):\n",
    "        if num_line == 0:\n",
    "            continue\n",
    "        data = line.strip().split('\\t', 1)\n",
    "        doc_id = int(data[0])\n",
    "        if len(data) == 1:\n",
    "            title = ''\n",
    "        else:\n",
    "            title = data[1]\n",
    "        doc_to_title[doc_id] = title\n",
    "print (len(doc_to_title))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('train_groups.csv')\n",
    "traingroups_titledata = {}\n",
    "for i in range(len(train_data)):\n",
    "    new_doc = train_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    target = new_doc['target']\n",
    "    title = doc_to_title[doc_id]\n",
    "    if doc_group not in traingroups_titledata:\n",
    "        traingroups_titledata[doc_group] = []\n",
    "    traingroups_titledata[doc_group].append((doc_id, title, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_train = []\n",
    "X_train = []\n",
    "groups_train = []\n",
    "for new_group in traingroups_titledata:\n",
    "    docs = traingroups_titledata[new_group]\n",
    "    for k, (doc_id, title, target_id) in enumerate(docs):\n",
    "        y_train.append(target_id)\n",
    "        groups_train.append(new_group)\n",
    "        all_dist = []\n",
    "        words = set(title.strip().split())\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j, target_j = docs[j]\n",
    "            words_j = set(title_j.strip().split())\n",
    "            all_dist.append(len(words.intersection(words_j)))\n",
    "        X_train.append(sorted(all_dist, reverse=True)[0:15]    )\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "groups_train = np.array(groups_train)\n",
    "print (X_train.shape, y_train.shape, groups_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберите размер батча для обучения. Линейная модель не должна учиться дольше нескольких минут. \n",
    "\n",
    "Не забывайте использовать скейлер!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбейте данные на обучение и валидацию. Подберите параметры C, alpha, max_epoch, model_type на валидации (Вы же помните, как правильно в этой задаче делать валидацию?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Подберите порог линейной модели, по достижении которого, Вы будете относить объект к классу 1. Вспомните, какую метрику мы оптимизируем в соревновании.  Как тогда правильно подобрать порог?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С лучшими параметрами на валидации сделайте предсказание на тестовом множестве, отправьте его на проверку на платформу kaggle. Убедитесь, что Вы смогли побить public score первого бейзлайна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** При сдаче домашки Вам необходимо кроме ссылки на ноутбук показать Ваш ник на kaggle, под которым Вы залили решение, которое побило Handmade baseline. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Фидбек (бесценно)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Какие аспекты обучения линейных моделей Вам показались непонятными? Какое место стоит дополнительно объяснить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше ответ здесь***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Здесь Вы можете оставить отзыв о этой домашней работе или о всем курсе.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** ВАШ ОТЗЫВ ЗДЕСЬ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "402px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}